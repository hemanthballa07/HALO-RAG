# Human Evaluation Annotation Guide

## Overview

This directory contains samples for human evaluation of the HALO-RAG system. Annotators are asked to evaluate whether the generated answers are supported by the provided context.

## Files

- `human_eval_samples.csv`: CSV file with 100 samples for annotation
- `README.md`: This annotation guide

## Annotation Instructions

### Task

For each sample, evaluate whether the **generated answer** is supported by the **context** provided.

### Labels

Use one of the following three labels:

1. **SUPPORTED**: The generated answer is fully supported by the context. All claims in the answer can be verified against the context.
2. **CONTRADICTED**: The generated answer contradicts information in the context, or contains factual errors not supported by the context.
3. **NO EVIDENCE**: The context does not provide sufficient information to verify or contradict the generated answer.

### Annotation Process

1. **Read the question**: Understand what is being asked.
2. **Read the context**: Review the provided context passages.
3. **Read the generated answer**: Review the answer generated by the system.
4. **Compare answer to context**: Check if all claims in the answer are supported by the context.
5. **Assign label**: Choose the appropriate label (SUPPORTED, CONTRADICTED, or NO EVIDENCE).
6. **Add notes (optional)**: Add any relevant notes or explanations.

### Label Definitions

#### SUPPORTED

**Criteria**:
- All factual claims in the generated answer are present in the context
- The answer accurately reflects the information in the context
- No contradictions or factual errors

**Example**:
- **Question**: "What is the capital of France?"
- **Context**: "Paris is the capital of France. It is located in the north-central part of the country."
- **Generated Answer**: "Paris is the capital of France."
- **Label**: SUPPORTED

#### CONTRADICTED

**Criteria**:
- The generated answer contradicts information in the context
- The answer contains factual errors not supported by the context
- The answer makes claims that are explicitly contradicted by the context

**Example**:
- **Question**: "What is the capital of France?"
- **Context**: "Paris is the capital of France. It is located in the north-central part of the country."
- **Generated Answer**: "Lyon is the capital of France."
- **Label**: CONTRADICTED

#### NO EVIDENCE

**Criteria**:
- The context does not provide sufficient information to verify the answer
- The answer may be correct, but cannot be verified from the context
- The answer addresses the question, but the context is irrelevant or insufficient

**Example**:
- **Question**: "What is the capital of France?"
- **Context**: "France is a country in Europe. It has a rich cultural history."
- **Generated Answer**: "Paris is the capital of France."
- **Label**: NO EVIDENCE (answer may be correct, but context doesn't provide evidence)

### Annotation Guidelines

1. **Be strict with SUPPORTED**: Only label as SUPPORTED if all claims are clearly supported by the context.
2. **Be careful with CONTRADICTED**: Only label as CONTRADICTED if there is an explicit contradiction or clear factual error.
3. **Use NO EVIDENCE for uncertainty**: If you're unsure whether the answer is supported, use NO EVIDENCE.
4. **Consider partial support**: If the answer is partially supported but contains unsupported claims, label as CONTRADICTED or NO EVIDENCE.
5. **Ignore answer quality**: Focus on factual support, not answer quality or completeness.

### CSV Format

The CSV file contains the following columns:

- **id**: Sample ID (e.g., "sample_001")
- **question**: The question being asked
- **context**: Context passages provided to the system
- **generated_answer**: Answer generated by the HALO-RAG system
- **gold_answer**: Ground truth answer (optional, for reference)
- **auto_label**: Automatic label assigned by the verification system
- **human_label**: Label assigned by human annotator (to be filled in)
- **notes**: Optional notes from annotator

### Filling in the CSV

1. Open `human_eval_samples.csv` in a spreadsheet editor (Excel, Google Sheets, etc.)
2. For each row:
   - Read the question, context, and generated answer
   - Assign a label in the `human_label` column (SUPPORTED, CONTRADICTED, or NO EVIDENCE)
   - Optionally add notes in the `notes` column
3. Save the CSV file

### Quality Control

- **Consistency**: Ensure consistent labeling across similar samples
- **Accuracy**: Double-check labels for ambiguous cases
- **Completeness**: Ensure all samples have human_label filled in
- **Notes**: Add notes for ambiguous or edge cases

## Evaluation Metrics

After annotation, the scorer script will compute:

- **Percent Match**: Percentage of samples where human_label matches auto_label
- **Cohen's κ**: Inter-annotator agreement coefficient (if multiple annotators)
- **Per-label Agreement**: Agreement for each label (SUPPORTED, CONTRADICTED, NO EVIDENCE)

### Expected Agreement

- **Target**: ≥ 0.85 agreement between human and auto labels
- **Cohen's κ**: ≥ 0.70 (substantial agreement)

## Example Annotations

### Example 1: SUPPORTED

```
Question: "What is the population of Paris?"
Context: "Paris is the capital of France. It has a population of approximately 2.1 million people within the city limits."
Generated Answer: "Paris has a population of about 2.1 million people."
Human Label: SUPPORTED
Notes: Answer is fully supported by context.
```

### Example 2: CONTRADICTED

```
Question: "What is the population of Paris?"
Context: "Paris is the capital of France. It has a population of approximately 2.1 million people within the city limits."
Generated Answer: "Paris has a population of 5 million people."
Human Label: CONTRADICTED
Notes: Answer contradicts the context (2.1M vs 5M).
```

### Example 3: NO EVIDENCE

```
Question: "What is the population of Paris?"
Context: "Paris is the capital of France. It is known for its art, culture, and history."
Generated Answer: "Paris has a population of about 2.1 million people."
Human Label: NO EVIDENCE
Notes: Context doesn't provide population information.
```

## Troubleshooting

### Ambiguous Cases

- **Partial support**: If answer is partially supported, use NO EVIDENCE or CONTRADICTED
- **Unclear context**: If context is unclear, use NO EVIDENCE
- **Multiple interpretations**: Choose the most appropriate label based on the majority of claims

### Common Issues

- **Answer quality vs. support**: Focus on factual support, not answer quality
- **Completeness**: Answer doesn't need to be complete, just supported
- **Relevance**: Context should be relevant to the question

## Contact

For questions or issues with annotation, please contact the project maintainers.

## References

- HALO-RAG Project Proposal
- Evaluation Metrics Documentation
- Annotation Guidelines Best Practices

